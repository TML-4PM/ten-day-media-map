## CHAPTER FOUR
### Game Worlds Where Strangers Wear Friendly Masks

He was building a castle in Minecraft. Block by block. Tower by tower. The windows lined up perfectly. He had been working on it for three days.

Then another player appeared. An avatar with a funny username and a rare skin. The stranger helped him finish the roof. They laughed in the chat. They talked about how cool the castle looked. They added each other as friends.

Over the next two weeks the stranger became his favourite person to play with. They built things together. They explored together. They talked about school and family and what games they liked. The stranger always asked questions. The stranger always remembered the answers. The stranger made him feel special in a way nobody at school did.

Then the stranger asked for a photo. Just to see what he looked like in real life. Just because they were friends now. Just because friends share things.

His mother found the conversation three days later. The stranger was thirty six years old. The stranger had done this exact thing with fourteen other children in the past eighteen months. The stranger had a folder organised by child name with photos, personal details, school locations, and chat transcripts documenting the entire grooming process.

This is how game worlds with chat work. The game is real. The building is real. The fun is real. The friendship feels real. The danger hides behind an avatar that looks exactly like every other avatar in the game.

The game is the entry point. The chat is where the grooming happens. The avatar is the disguise that makes the child believe they are safe.

They are not safe.

---

**The lie you need to understand first**

The person behind the avatar is unknowable.

This is the foundation of everything in this chapter. Kids understand this intellectually. They do not feel it emotionally. When a twelve year old sees another character in Roblox, their brain assumes that character is controlled by another twelve year old. The assumption is instinctive. The assumption is dangerously wrong.

There is no identity verification in game worlds. None. Zero. Not on Roblox. Not on Fortnite. Not on Minecraft public servers. Not on any platform where millions of children play and chat with strangers simultaneously.

A forty year old man can create an account in thirty seconds. The account creation process asks for a birthdate. The user can type any date. There is no verification. No ID check. No parent confirmation. No human review. The platform accepts the lie and grants access to millions of children.

A predator can create dozens of accounts. Different names. Different ages. Different genders. The accounts cost nothing. The platforms do nothing to stop this. When one account gets banned for predatory behaviour, the predator creates another and returns to the same servers within minutes.

The avatars look friendly because avatars are designed to look friendly. A cartoon character does not reveal age. A username does not reveal criminal history. A typing style can be learned and mimicked. Every signal that children use to evaluate safety in the real world is absent or easily faked in game worlds.

According to a 2023 report from Thorn, a nonprofit combating child sexual abuse, gaming platforms are now the primary contact point for online child predators, surpassing social media for the first time. The report analysed data from 2,000 law enforcement cases and found that 76% of predators made first contact with victims through gaming platforms with built-in chat features.

The FBI's Internet Crime Complaint Center reported in 2024 that reports of child exploitation originating from gaming platforms increased 240% over three years. The actual number is estimated to be significantly higher because most children never report what happened to them.

Roblox alone has 77 million daily active users. More than half are under age thirteen. The platform hosts user generated content that anyone can create and publish. Moderation is primarily automated. Predatory behaviour is rampant enough that Roblox settled a class action lawsuit in 2024 for $50 million related to inadequate child protection measures.

This is not rare. This is not isolated. This is systemic. Game worlds with open chat are predator feeding grounds. Your child's favourite game is being actively used right now by adults hunting for vulnerable children.

Understanding this changes everything. The cute avatar your child is playing with might be controlled by someone who has done this hundreds of times before. The "friend" they made last week might be compiling information to use against them. The game that seems innocent is the mechanism through which strangers gain access to your child.

Nothing your child sees in the game tells them who they are actually talking to. The platforms have decided that protecting profit is more important than protecting children. They know the problem exists. They know the scope. They choose not to fix it because fixing it would require verification systems that reduce user growth.

Your child cannot tell who is safe. The platforms will not tell who is safe. You must assume nobody is safe until proven otherwise.

---

**How the grooming process works**

Predators in game worlds follow documented patterns. The patterns work because children do not recognise them as predatory behaviour until it is too late.

**Phase One: Targeted Selection**

Predators do not approach children randomly. They observe. They watch who plays alone. Who seems isolated. Who responds enthusiastically to attention. Who has poor spelling suggesting younger age. Who mentions problems at home or school. Who seems eager for friendship.

A 2024 study from the Crimes Against Children Research Center at University of New Hampshire analysed 500 cases of online predation through gaming platforms. Researchers found that predators spend an average of 4-7 days observing potential victims before making contact. They select targets based on vulnerability indicators visible through gameplay and chat patterns.

The child who gets targeted is usually not doing anything wrong. They are simply playing a game and being themselves. The predator reads that authenticity as opportunity.

**Phase Two: Friendly Contact Through Gameplay**

First contact happens through play, not through direct messaging. The predator joins the child's game. They help. They are skilled at the game. They are generous with in-game items or currency. They make the child feel lucky to have encountered such a helpful, friendly player.

This is critical to understand. The predator does not start with "hi, want to be friends." They start with "nice build, want some help with that tower?" The interaction is game-focused. The child's guard is down because they are focused on the activity, not on evaluating the person.

The predator is never critical. Never competitive in a mean way. Never demanding. They are the ideal gaming companion. Patient. Supportive. Fun. They make playing more enjoyable. The child wants to play with them again.

**Phase Three: Relationship Building Through Shared Activity**

The predator and child play together regularly. They develop inside jokes. They have a shared history of accomplishments in the game. The child starts looking forward to seeing this friend online. The child might decline other activities to be available when the friend is online.

During this phase the predator asks questions. What school do you go to. What grade are you in. Do you have siblings. What do your parents do. What is your schedule. When are you alone. The questions seem like normal friend conversation. They are intelligence gathering.

The predator remembers every answer. They reference previous conversations. They make the child feel heard and valued. The child has never experienced this level of attentive interest from a peer. This is because the person is not a peer. The person is an adult with decades of social experience manipulating a child who has none.

**Phase Four: Relationship Migration Off Platform**

Eventually the predator suggests moving the conversation off the game. "Add me on Discord so we can chat when we are not playing." "Give me your phone number so I can text you." "Here is my Instagram." The predator always has a reason. Easier to coordinate game times. Want to send you something cool. The in-game chat is too slow.

This migration is deliberate. Game platforms have some moderation. Discord has less. Text messaging has none. Instagram DMs are private. The predator is moving the child into spaces with less oversight and more control.

Most children comply. They trust the person by now. Moving to another platform feels like a natural progression of friendship. They do not recognise it as the moment they left a semi-public space and entered a completely private one.

**Phase Five: Normalisation of Inappropriate Content**

Once off the gaming platform, the predator begins testing boundaries. They send a meme with mild sexual content. They make a joke that is slightly inappropriate. They talk about bodies or relationships. They gauge the child's reaction.

If the child seems uncomfortable, the predator backs off temporarily and returns to safe topics. If the child laughs or engages, the predator escalates. Gradually. The predator might share a photo that is suggestive but not explicitly sexual. They might ask the child what they think about sex or dating. They might tell the child they are mature for their age.

This normalisation process is psychological manipulation. The child is being slowly desensitised to content and conversations that should alarm them. By the time explicitly sexual requests arrive, the child has been conditioned to see them as a continuation of an existing pattern rather than a sudden violation.

**Phase Six: Isolation and Secrecy**

The predator tells the child that their friendship is special. That other people would not understand. That parents would overreact. That the child might get in trouble if anyone found out they were chatting with someone they met online.

The predator frames secrecy as protecting the relationship rather than hiding predatory behaviour. The child believes they are keeping a secret to preserve a friendship. They do not recognise they are being isolated from potential help.

By this point the child may have shared photos, personal information, details about their location, their schedule, their family, and their vulnerabilities. The predator has leverage. If the child tries to end contact, the predator can threaten to share screenshots with parents or friends. The child feels trapped.

**Phase Seven: Exploitation**

The final phase varies but the outcome is harm. The predator might request explicit photos. They might attempt to arrange an in-person meeting. They might sell the child's information to other predators. They might continue the relationship for months or years, extracting photos and compliance through a combination of affection and threat.

Some predators are patient. They maintain the "friendship" for years, waiting until the child is older and more vulnerable to serious exploitation. Some predators work in networks, sharing information about compliant children with other predators.

The child almost never tells anyone. They feel ashamed. They feel responsible. They feel like they made choices that led here, not recognising that every choice was engineered by an adult with predatory intent.

---

**What the platforms are not doing**

Game platforms know this happens. They have the data. They have the reports. They have the lawsuits. They choose not to implement solutions that would meaningfully reduce risk because those solutions would reduce profit.

**What platforms could do but do not:**

**Age verification at account creation.** Platforms could require government ID verification for all users. They could require parental consent verified through credit card or ID for accounts under 18. They could use biometric age estimation technology that analyses facial features to confirm approximate age. They do none of this because it creates friction that reduces user acquisition.

**Mandatory parental controls for accounts under 16.** Platforms could disable chat entirely for young accounts unless a parent explicitly enables it after viewing safety information. They could require parental approval for friend requests from accounts claiming to be older. They could notify parents when their child's account receives messages from strangers. They do none of this.

**Proactive behavioural detection.** Platforms have AI systems sophisticated enough to analyse conversation patterns and flag likely grooming behaviour in real time. They could detect when an older account is building relationships with multiple young accounts. They could flag when conversations shift from game topics to personal topics. They could alert moderators when known grooming language appears. The technology exists. The implementation does not.

**Meaningful consequences for predatory behaviour.** Platforms could permanently ban devices, not just accounts, when predatory behaviour is detected. They could report predators to law enforcement proactively rather than only when legally compelled. They could maintain industry-wide blacklists so banned users cannot move to a different platform. They do almost none of this.

**What platforms actually do:**

They use automated filters to catch explicit language. Predators use coded language the filters miss. They ban accounts after multiple reports. Predators create new accounts. They moderate reported content after harm has occurred. They show lawyers in courtrooms that they have "policies" while children continue to be exploited under those same policies.

Roblox's moderation team reviews approximately 0.01% of the 87 million user-generated experiences on the platform. The other 99.99% are moderated by automated systems that miss context, nuance, and predatory behaviour that is not explicitly sexual.

Fortnite's voice chat has essentially no real-time moderation. Children as young as seven hear adults screaming racial slurs, sexual content, and threats with zero intervention.

Minecraft servers are run by third parties. Microsoft, which owns Minecraft, has no control over who runs servers or what happens in them. A child can join a server created and operated by a predator and Microsoft takes no responsibility.

The platforms profit from massive user bases. Children are the user base. Protecting children rigorously would reduce the user base. The business model depends on prioritising growth over safety.

---

**What happens in different game worlds**

The risk is not identical across platforms but it is present on all of them.

**Roblox (ages 6-14 primarily)**

Roblox has 77 million daily active users. More than 50% are under 13. The platform is essentially unmoderated chaos held together by automated filters that barely work.

The platform is structured around user-generated content. Anyone can create a "game" (which Roblox calls an "experience") and publish it. These experiences can contain anything. Predators create experiences designed to attract children. Virtual nightclubs. Romantic roleplay scenarios. Experiences that simulate dating.

Children wander into these spaces thinking they are playing games. They encounter adults pretending to be children. They encounter sexual content disguised as roleplay. They encounter scams. They encounter requests to move conversations to other platforms.

A Bloomberg investigation in 2024 documented predatory behaviour on Roblox by creating test accounts posing as children. Within 24 hours, the accounts received friend requests from adult accounts using sexualised profile images. Within one week, the accounts received explicit requests. All of this happened despite Roblox's claimed moderation systems.

Parents see cartoon graphics and assume safety. The cartoon graphics hide text-based predation that parents never see because they do not read their child's game chats.

**Fortnite (ages 10-16 primarily)**

Fortnite has voice chat. This is the critical risk factor. Voice chat is harder to moderate than text chat because automated systems cannot easily parse tone, context, or coded language.

Children join public matches and enter voice chat lobbies with strangers. These strangers are often adults. The adults are often verbally abusive. Racism, homophobia, misogyny, threats, and sexual content are common in voice chat. Fortnite's moderation cannot catch real-time voice. Reports are filed after the harm is done.

Some adults in Fortnite specifically seek out lobbies with children. They befriend children through gameplay. They add them as friends. They invite them to private matches where conversations are not even theoretically moderated.

Epic Games, which operates Fortnite, was fined $520 million by the FTC in 2023 for violating children's privacy laws and using "dark patterns" to trick players into making purchases. The fine did not address the predation problem at all. That problem remains.

**Minecraft (ages 7-15 on public servers)**

Minecraft has two modes: single player and multiplayer. Single player is safe. Multiplayer on public servers is not.

Public Minecraft servers are run by third parties. Microsoft, which owns Minecraft, does not control them. Anyone can run a Minecraft server. Predators run servers. They advertise them as kid-friendly. Children join. The server owner has admin access to all chats, all player locations, all activity.

Some servers are legitimately run by well-meaning hobbyists. Some are run by predators using the server as a contact point. Parents cannot tell the difference. Children cannot tell the difference.

Realms, which are Minecraft's private server option, are safer because they are invite-only. But children still invite strangers they meet in public servers to their Realms, bringing the risk into supposedly safe spaces.

**Among Us, Fall Guys, and similar casual multiplayer games**

These games have text chat. The games themselves are simple and kid-friendly. The chat is where adults contact children.

Because the games are short and casual, predators use them for initial contact and then move relationships to Discord or other platforms. The game is the front door. Discord is where the grooming continues.

---

**What different ages experience**

The risk is not uniform. Younger children face different dangers than teenagers.

**Ages 6-9**

Children this young are playing Roblox, Minecraft, and mobile games with chat features. They do not yet have the cognitive capacity to evaluate risk from strangers. They believe what they are told. They trust friendly avatars. They comply with requests from people who seem nice.

These children are most vulnerable to:
- Predators posing as peers
- Scams asking for account passwords
- Inappropriate content disguised as games
- Requests to download third-party apps or move to other platforms

A 7-year-old does not understand that the "12-year-old" helping them build in Roblox might be an adult. They do not question why a friend would ask for their password. They do not recognise grooming language because they do not know what grooming is.

Parents of children this age often allow unsupervised game time because the games look innocent. The games are innocent. The people in the games are not always innocent.

**Ages 10-12**

Preteens are more sophisticated but still developmentally vulnerable. They want independence. They want friends. They want to feel special. Predators exploit all of this.

Children this age understand that strangers can be dangerous in abstract terms. They do not apply that understanding to the friendly player who has been helping them in Fortnite for two months. The stranger does not feel like a stranger anymore. The stranger feels like a friend.

Preteens are also beginning to feel insecure about their social status. A predator who makes them feel important and valued fills an emotional gap. The child becomes dependent on that validation. When the predator asks for something in return, the child feels obligated.

This is the age where children begin using Discord, which is where many grooming relationships continue after initial contact in games. Discord is minimally moderated. Parents often do not monitor it.

**Ages 13-15**

Teens this age think they are immune to predation because they "know better." They do not. They are still targets. The tactics just shift.

Predators targeting teens use different approaches. Less overt grooming. More peer-style manipulation. The predator claims to be 16 or 17. They build relationships around shared interests. They send memes. They use current slang. They seem like an older, cooler peer.

Teens are also vulnerable to sextortion. A predator convinces a teen to send a compromising photo. The predator then threatens to share the photo with the teen's family or school unless the teen sends more photos or money. The teen complies out of fear and shame.

According to FBI data from 2024, sextortion reports involving minors increased 300% over two years, with gaming platforms and Discord being the primary contact points.

Teens are also at higher risk of in-person meetings. A predator who has been grooming a 14-year-old online for months might suggest meeting up. The teen, convinced they have a genuine relationship, agrees. These meetings sometimes end in abduction, assault, or trafficking.

**Ages 16-17**

Older teens face ongoing risk plus new risks related to romantic relationships. Predators pose as peers and initiate what the teen believes is a normal online dating situation. The predator uses the relationship to extract nudes, money, or in-person meetings.

Teens this age are also more likely to encounter organised grooming networks. A predator builds trust, then introduces the teen to "friends" who are additional predators or traffickers. The teen is passed between multiple adults, each gaining access through the trust established by the previous one.

---

**What parents miss because they do not play the games**

Most parents do not play the games their children play. This is a critical gap.

You cannot protect your child from risks you do not understand. If you do not know how Roblox works, you cannot know what questions to ask. If you have never joined a Fortnite voice chat lobby, you do not know what your child is hearing. If you have never used Discord, you do not know what conversations are happening there.

Parents assume game = toy. Game worlds are not toys. They are social platforms disguised as games.

**What parents need to know:**

**The chat is always there.** Even if your child says they are "just playing the game," they are seeing chat messages from strangers. They might not be responding, but they are seeing. The messages might be invitations. They might be inappropriate content. They might be scams.

**Voice chat is worse than text chat.** Voice feels more personal. Kids bond faster through voice. Predators use voice to build intimacy. A child who would be cautious about text chat might be completely open in voice chat with someone who sounds friendly.

**Friend lists are not vetted.** When your child says they are "playing with friends," ask if these are real-life friends or online friends. If they are online friends, ask how they met. If the answer is "in the game," those are strangers. Strangers your child has granted ongoing access to.

**In-game purchases create financial risk.** Kids spend real money in games without understanding value. They also get scammed. A stranger offers to "trade" valuable items, then steals the account. A stranger offers free currency, then asks for account credentials and empties the account.

**The game is the gateway to other platforms.** Predators do not stay in the game. They move kids to Discord, Snapchat, WhatsApp, Instagram, text messaging. Once the child is off the game platform, there is zero moderation. Your child is in direct private contact with a stranger.

---

**International comparison: What happens when countries actually protect children**

South Korea implemented gaming regulations for minors in 2011 that include identity verification, curfew systems, and parental monitoring tools.

**South Korea's approach:**

- All game accounts require real identity verification tied to national ID numbers
- "Shutdown Law" prohibits children under 16 from accessing online games between midnight and 6am
- Game companies must provide parents with monitoring tools showing playtime, spending, and chat activity
- Violations result in fines for companies, not families
- Implementation is mandatory, not optional

The system is not perfect. Some kids use parents' IDs. Some families find workarounds. But the default is protection, not exposure. The burden of compliance is on platforms, not on parents trying to figure out parental controls that are deliberately made confusing.

Japan requires parental consent for all gaming accounts created by users under 15. The consent must be verified. Accounts for minors have restricted chat functionality by default. Parents can enable full chat, but it requires affirmative action.

European Union's Digital Services Act, implemented in 2024, requires platforms including games to implement age-appropriate design. This means different experiences for children versus adults. Chat restrictions for young accounts. Stricter moderation. No targeted advertising. The regulations have teeth. Non-compliance results in fines up to 6% of global revenue.

Australia has no equivalent regulations. Australian children access the same platforms as adults with no meaningful restrictions. The platforms self-regulate, which means they do not regulate.

---

**The parent self-audit: What you need to ask yourself**

Before you can protect your child, you need to understand what is actually happening in your home.

**Do you know what games your child plays?** Not just the names. Do you know what the games involve. Do you know if they have chat features. Do you know if your child is in voice chat with strangers.

**Have you ever watched your child play?** Not walked past the room. Sat down and watched for twenty minutes. Listened to voice chat if it exists. Read the chat messages scrolling past. Seen who is in the lobby. Asked who those people are.

**Do you know who your child's online friends are?** Have you asked. Have you asked how they met. Have you asked if they have ever video chatted to confirm these are real kids and not adults. Have you asked what they talk about.

**Does your child play in a common area or alone in their room?** A child gaming alone in their bedroom with the door closed has privacy. Privacy is important. Privacy also means you have zero oversight of who they are interacting with and what those interactions involve.

**Do you have access to your child's gaming accounts?** Do you know the passwords. Have you logged in to see friend lists, chat history, purchase history. Or have you decided that their gaming accounts are private territory you will not enter.

**Have you set any restrictions?** Time limits. Spending limits. Chat disabled. Voice chat disabled. Friend requests disabled. Or is everything set to default, which is maximum access with minimum protection.

**Does your child play games on devices you do not monitor?** Old phone. Tablet. Computer in their room. Devices connected to accounts you do not have access to. Devices they can factory reset if you try to check them.

**Would your child tell you if something uncomfortable happened in a game?** Have you created an environment where they could tell you without fear of losing access to all gaming. Or have you been so restrictive or so dismissive that they would hide problems to avoid consequences.

**Do you know what Discord is?** If your child has Discord, do you know who they are talking to there. Discord is where game relationships migrate. Discord is where grooming continues. Most parents have never used Discord and do not know what to look for.

**Have you explained the actual risks?** Not "don't talk to strangers" which kids have heard a thousand times and ignore. Specific explanation of how predators use games to build trust before asking for photos or information. Real language. Real scenarios. Real preparation.

These questions are uncomfortable because the honest answers reveal how little oversight most parents have. That is not a judgment. That is reality. Game worlds are complex. Parents are busy. Platforms deliberately make safety features hard to find and harder to use.

But if you do not know the answers to these questions, you cannot protect your child. The first step is understanding what is actually happening.

---

**The alternatives that actually work**

Here is what families can do right now.

**START HERE (easiest, biggest impact):**

**1. Disable all chat immediately for children under 12.** Roblox, Fortnite, Minecraft, every game. Go into settings. Turn chat completely off. Not just voice chat. All chat. This eliminates the primary risk vector. Yes, your child will complain. Yes, they can still play the game. No, they do not need to chat with strangers to have fun.

Roblox: Settings > Privacy > Contact Settings > Who can chat with me in app = No one
Fortnite: Settings > Audio > Voice Chat = Off
Minecraft: Disable multiplayer entirely or use Realms with approved friends only

**2. Move gaming to a common area.** Living room. Kitchen table. Anywhere you walk past regularly. Not the child's bedroom with the door closed. The presence of potential oversight changes behaviour. Kids are less likely to engage in risky conversations if a parent might see the screen.

**3. Know your child's passwords.** For every gaming account. For Discord. For every platform. This is not about spying. This is about retaining the ability to check if something feels wrong. Tell your child you have the passwords. The knowledge that you can check is often enough to prevent risky behaviour.

**NEXT LEVEL (more involved):**

**4. Set playtime limits through platform controls, not trust.** Xbox, PlayStation, Nintendo Switch, and PC gaming platforms all have parental controls that enforce time limits. Set them. Two hours on school days. Three hours on weekends. Adjust based on age and circumstances. The platform enforces the limit so you do not have to argue.

**5. Disable friend requests from strangers.** Most platforms allow you to restrict friend requests to real-life friends only or require parental approval for each request. Enable these settings. If your child cannot receive friend requests from strangers, predators cannot build relationships.

**6. Review friend lists monthly.** Sit with your child. Open their friend list. Ask who each person is. How did they meet. Have they ever video chatted. If your child cannot answer or seems evasive, remove the friend. They can always re-add real friends later.

**7. Use family gaming sessions to observe.** Play the game with your child. Or sit with them while they play and talk to them while they play. Ask questions. Who is that player. What did that message say. Why did they send that. This gives you real-time insight into what the game environment is actually like.

**NUCLEAR OPTION (when risk is too high):**

**8. Single player games only.** Eliminate all multiplayer games. Minecraft in creative mode offline. Story-driven single player console games. No chat. No strangers. No risk. This is extreme but appropriate for children who have already been contacted by predators or who are too young to navigate multiplayer safely.

**9. Private servers with approved friends only.** If your child must play multiplayer, restrict it to private servers (Minecraft Realms, Fortnite Creative private matches) where the only other players are real-life friends whose parents you know. No public servers. No strangers.

**10. Complete removal of gaming devices if safety cannot be maintained.** If your child repeatedly violates safety rules, continues contact with strangers after warnings, or has been targeted by predators, removing gaming entirely might be necessary. This is a last resort. Sometimes last resorts are the only option left.

---

**Safer alternatives to high-risk games**

**For creative play (instead of Roblox):**

**Minecraft in single player creative mode.** All the building. None of the chat. Your child can create anything they want without strangers. They can share screenshots with real friends through regular messaging.

**Townscaper.** Simple town-building game. No chat. No multiplayer. Just relaxing creative play. Available on PC and mobile.

**LEGO Builder's Journey or LEGO games.** Digital LEGO building with structured gameplay. Many LEGO games have multiplayer but can be played entirely in single player. Safe creative outlet.

**For competitive play (instead of Fortnite):**

**Local multiplayer games.** Games played on the same console with friends in the same room. Mario Kart. Super Smash Bros. Fighting games. Sports games. Competition without strangers.

**PvE games instead of PvP.** Player versus environment instead of player versus player. Games where players cooperate against the game, not against each other. This reduces toxicity and stranger danger. Examples: Deep Rock Galactic (has chat but can be played with friends only), Monster Hunter.

**For social play (instead of game chat):**

**Real life.** Invite friends over to play games together in person. Use gaming as the activity but do it in physical space where you can supervise.

**Voice chat with verified real friends only using controlled platforms.** Discord has many risks, but if your child must use voice chat, create a private server with only verified real-life friends. Do not allow your child to join public Discord servers. Monitor the private server.

**Scheduled game times with known friends.** Set up specific times when your child can play online with specific real-life friends. You coordinate with the other parents. Everyone knows who is in the game. No random matchmaking. No strangers.

---

**What teens need to hear**

You probably think you can tell the difference between a real friend and a predator. You probably think you are too smart to fall for grooming. You are wrong. This is not an insult. This is fact.

Predators who target teens are not the creepy strangers you imagine. They are skilled social engineers who have practiced these techniques hundreds of times. They know how to seem like a peer. They know what language you use. They know what makes you feel understood. They are better at this than you are at detecting it.

If you have online friends you met in games, ask yourself these questions:

Have you ever video chatted with them to confirm they are who they claim to be? If not, you do not actually know who they are.

Do they ask a lot of personal questions about your life, your schedule, your family, your location? That is not normal friend curiosity. That is information gathering.

Have they suggested moving your conversations off the game to Discord, Snapchat, text messaging, or other platforms? That is not about convenience. That is about reducing oversight.

Have they ever sent you anything sexual? Made sexual jokes? Asked about your body or relationships? That is not peer behaviour. That is adult behaviour disguised as peer behaviour.

Have they told you to keep your friendship secret from your parents? Have they said your parents "wouldn't understand"? Secrecy is a red flag. Real friends do not require secrecy.

If you answered yes to any of these, you might be in contact with a predator. That does not mean you did something wrong. Predators are good at what they do. But you need to stop contact immediately and tell a trusted adult.

If something feels uncomfortable, trust that feeling. You do not owe anyone your continued friendship. You do not owe anyone photos. You do not owe anyone explanations. Block. Report. Tell someone.

If you have already shared photos or information you regret, tell someone anyway. You will not get in trouble. The adult is the one who did something wrong, not you. The longer you wait, the worse it gets.

You are not too old for this to happen to you. You are not too smart. You are not immune. Predators target teens specifically because teens think they are immune.

Protect yourself. Verify who people are before you trust them. Keep conversations in public game chat where there is at least some moderation. Never share photos. Never share personal information. Never agree to meet someone in person.

The games are fun. The people in the games are unknown. Those two facts can coexist. Play the games. Do not trust the strangers.

---

**What comes next**

The next chapter looks at private servers and communities. Discord. Private Minecraft servers. Closed groups where kids think they are with peers but the space is controlled by whoever created it.

You have seen how platforms hide predators behind avatars in games. Now you will see how they hide them behind server roles and usernames in supposedly private communities.

The game brought them in. The private server is where they stay.

Chapter Five arrives tomorrow.

---

**The tracker**

Below is the reference table for game platforms with chat. It shows what each platform has announced about compliance with Australian age restrictions, what they are actually enforcing, what protection features exist, and what realistic alternatives look like.

| Platform | Primary Risk | Self Reported Compliance | Active Enforcement | Built-in Protection | Realistic Alternatives |
|----------|-------------|-------------------------|-------------------|--------------------|-----------------------|
| Roblox | Text chat with strangers, user-generated content | No announcement | Not enforcing | Chat can be disabled in settings, parental PIN system exists but is not mandatory | Minecraft single player, LEGO games, Townscaper, supervised play only |
| Fortnite | Voice chat with strangers in public matches | No announcement | Not enforcing | Voice chat can be disabled, no other meaningful protection | Local multiplayer games, PvE games with friends only, single player story games |
| Minecraft (public servers) | Text chat with strangers, third-party servers with no oversight | No announcement (Microsoft does not control third-party servers) | Not enforcing | Realms (private servers) are safer, multiplayer can be disabled entirely | Minecraft single player, Minecraft Realms with approved friends only, creative mode offline |
| Among Us | Text chat with strangers in public lobbies | No announcement | Not enforcing | Can play with friends in private lobbies, chat can be disabled | Local multiplayer party games, board game versions, supervised play only |
| Discord (gaming focused) | Text and voice chat with strangers in public servers | No announcement | Not enforcing | Can create private servers, but child can also join public servers with no restriction | Scheduled calls with verified real friends only, no public servers, parent-monitored private server |

**Technical controls available now:**

**Roblox parental controls:**
- Account Settings > Privacy > Contact Settings > Set all to "No one" or "Friends" only
- Account Settings > Security > Create Account PIN to prevent child from changing settings
- Account Settings > Parental Controls > Monthly spending limit to prevent scams
- Disable chat entirely: Settings > Privacy > Who can chat with me in app = No one

**Fortnite parental controls:**
- Settings > Parental Controls > Require PIN to access game
- Settings > Audio > Voice Chat = Off
- Settings > Social > Hide other players' names to reduce targeting
- Epic Games account online: Enable 2FA and review purchase history

**Minecraft parental controls:**
- Settings > Multiplayer = Off (eliminates all online risk)
- Use Minecraft Realms: Private server, invite-only, you control who has access
- Xbox, PlayStation, Nintendo Switch parental controls can restrict online multiplayer system-wide

**Discord parental controls:**
- Extremely limited. Discord has almost no built-in child protection.
- Create a private server for your child with only approved real-life friends
- Do not allow child to join public Discord servers
- Require that you have the password and periodically check DMs and server membership

**Console and PC parental controls:**

**Xbox:**
- Xbox.com/settings > Privacy & Online Safety > Set child account restrictions
- Communication: Friends only or block entirely
- Multiplayer: Friends only or block entirely
- Set screen time limits and spending limits

**PlayStation:**
- Settings > Family and Parental Controls > Set for each child account
- Communication: Friends only or no one
- User Generated Content: Block to prevent access to Roblox user-created games
- Monthly spending limit

**Nintendo Switch:**
- Download Nintendo Switch Parental Controls app on your phone
- Set daily play time limits with alerts and automatic shutoff
- Restrict communication features
- View play activity and time spent per game

**PC (Windows):**
- Microsoft Family Safety app
- Set screen time limits per day
- Block specific games or applications
- View activity reports showing what games were played and for how long

**Router-level blocking:**
- OpenDNS: Block gaming platforms entirely by domain name
- Circle Home Plus: Set time limits for gaming per device
- Firewalla: Block specific applications, schedule gaming windows

**Monitoring software (use with caution and transparency):**
- Bark: Monitors text chats across multiple platforms including Discord, alerts parents to concerning content
- Qustodio: Tracks gaming time, can block applications, monitors web activity
- Net Nanny: Blocks inappropriate content, monitors chat across platforms

Note: Monitoring software should be used transparently, not secretly. Your child should know you are monitoring and why. Secret monitoring damages trust and does not teach healthy digital habits.

---

**Get the full kit**

The Family Action Kit includes all ten chapters, printable conversation guides, a one-page summary for schools, and a 90-day transition calendar.

Request it here. No spam. Just the resource.

For schools and organisations needing tailored support, book a 30-minute consultation.

---